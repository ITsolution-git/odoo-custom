-
  In order to test hr evaluation plan.
-
  I set the survey in open state.
-
  !python {model: survey}: |
       self.survey_open(cr, uid, [ref("survey_2")], context)
-
  I check that state of survey is Open.
-
  !assert {model: survey, id: survey_2, severity: error, string: Survey should be in OPen state}:
      - state == 'open'
-
  I start the evaluation process by click on "Start Evaluation" button.
-
  !python {model: hr_evaluation.evaluation}: |
     self.button_plan_in_progress(cr, uid, [ref('hr_evaluation_evaluation_0')])
-
  I check that state is open.
-
  !assert {model: hr_evaluation.evaluation, id: hr_evaluation_evaluation_0, severity: error, string: Evaluation should be in open state}:
      - state == 'wait'
-
  I find a mistake on evaluation form. So I cancel the evaluation and again start it.
-
  !python {model: hr_evaluation.evaluation}: |
     self.button_cancel(cr, uid, [ref('hr_evaluation_evaluation_0')])
     self.button_draft(cr, uid, [ref('hr_evaluation_evaluation_0')])
     self.button_plan_in_progress(cr, uid, [ref('hr_evaluation_evaluation_0')])
-
  I check that state is open.
-
  !assert {model: hr_evaluation.evaluation, id: hr_evaluation_evaluation_0, severity: error, string: Evaluation should be in open state}:
      - state == 'wait'
-
  I close this survey request by giving answer of survey question.
-
  !python {model: hr_evaluation.evaluation}: |
    evaluation = self.browse(cr, uid, ref('hr_evaluation_evaluation_0'))
    interview_obj=self.pool.get('hr.evaluation.interview')
    interview_obj.survey_req_done(cr, uid, [r.id for r in evaluation.survey_request_ids])
    for survey in evaluation.survey_request_ids:
        interview = interview_obj.browse(cr, uid, survey.id, context)
        assert interview.state == "done", 'survey must be in done state'
-
  I print the survey.
-
  !python {model: hr_evaluation.evaluation}: |
    evaluation = self.browse(cr, uid, ref('hr_evaluation_evaluation_0'))
    self.pool.get('hr.evaluation.interview').action_print_survey(cr, uid, [r.id for r in evaluation.survey_request_ids])
-
  I click on "Final Validation" button to finalise evaluation.
-
  !python {model: hr_evaluation.evaluation}: |
    self.button_final_validation(cr, uid, [ref("hr_evaluation_evaluation_0")])
-
  I check that state is "Final Validation".
-
  !assert {model: hr_evaluation.evaluation, id: hr_evaluation_evaluation_0}:
      - state == 'progress'
-
  Give Rating "Meet expectations" by selecting overall Rating.
-
  !record {model: hr_evaluation.evaluation, id: hr_evaluation_evaluation_0}:
    rating: '2'
-
  I close this Evaluation by click on "Done" button of this wizard.
-
  !python {model: hr_evaluation.evaluation}: |
    self.button_done(cr, uid, [ref("hr_evaluation_evaluation_0")])
-
  I check that state of Evaluation is done.
-
  !assert {model: hr_evaluation.evaluation, id: hr_evaluation_evaluation_0, severity: error, string: Evaluation should be in pending state}:
      - state == 'done'